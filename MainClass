import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
import scala.util.matching.Regex
import org.apache.spark._
import org.apache.spark.sql._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions.explode
import org.apache.spark.sql.functions._

object MainClass{
  def main(args: Array[String]){ // maim func
    val conf = new SparkConf().setAppName("Sample App") // std things
    conf.setMaster("local[4]") //для запуска на локалке

    val sc = new SparkContext(conf) // to read json
    //val sqlContext = new SQLContext(sc)
    val mySpark = SparkSession
      .builder()
      .appName("Spark SQL basic example")
      .config("spark.some.config.option", "some-value")
      .getOrCreate()
    import mySpark.implicits._ // function fot json

    val inputPath = "/home/arseny/Desktop/WikiFiles/AA_wiki_00"
    val inputFolder = "/Users/V/Desktop/BD_ass/files"
    val outputPath = "/home/arseny/Desktop/Vocabulary"
    val outputFolder = "/home/arseny/Desktop/Out"

    val regex2 = "(.*\\p{Punct}.*)" //регулярное выражение для удаления всех слов с пунктуацией
    val pat1 = """[\p{Punct}]""" // remove all punctuation
    val pat2 = """ +""" // remove spaces
/*
    val pats = (s:String) => {
      s.replaceAll(pat,null)
        .replaceAll(pat2,null)
        .replaceAll(pat3,null)
    }*/

    //читаем один файл, который состоит из нескольких элементов, считаем кол-во слов
    val readFull = mySpark.read.json(inputPath) // read json
    readFull.printSchema() // print schema of json
    readFull.createOrReplaceTempView("Files") // sql table to use sql language name Files

    //считаем все слова
    val full=mySpark.sql("select text from Files") // read column text
    val vocab = full.map(rows => rows.getString(0).toLowerCase()).rdd
      .flatMap(line => line.split(" "))
      .map(line => line.replaceAll(pat, ""))
      .map(line => line.replaceAll(pat2, ""))
      .map(line => line.replaceAll(pat3, ""))
      .map(word => (word, 1))
      .reduceByKey(_ + _)
      .sortBy(- _._2)
      .map(f => f._1 +"\t"+ f._2)
    vocab.saveAsTextFile(outputPath)

    //считаем TF
    val separate=mySpark.sql("select id, text from Files") // select 2 columns
    //val d = readFull.select('text).collect().map(rows => rows.getString(0))//.map(line => line.replaceAll(regex2, ""))
    val dss = separate.map(rows => (rows.getString(0), rows.getString(1).toLowerCase().split(" ").map(line => line.replaceAll(pat1, " ")).map(line => line.replaceAll(pat2, " ")))).toDF("id","text") //  (id, text) name of columns
    //val cc = ds.flatMap(line => line._2.split(" "))
    dss.printSchema()2
    val dss2 = dss.withColumn("text", explode($"text")) // (idd 1, word 1) (idd 1, word 2 )
      .groupBy("id","text")
      .count()
      .sort(asc("id"),desc("count"))

    dss2.printSchema()

    //считаем IDF
    val dss3 = dss2.groupBy("text").agg(countDistinct("id") as "idf")
    dss3.printSchema()

    //объединяем таблицы по одинаковым словам
    val dss4 = dss2.join(dss3, Seq("text"), "left")
      .withColumn("tf-idf", col("count") / col("idf"))

    dss4.printSchema()
    //val op = dss4.rdd.map(_.toString().replace("[","").replace("]", "")).saveAsTextFile(outputFolder)
    val op = dss4.write.json(outputFolder)

    //считываем все файлы из папки в один dataset и считаем слова
    //по сути нам надо создать такой файл для каждой строки/документа отдельно
    //и создать вектор считающий в скольких документах содержится слово

    /*val readAll = mySpark.read.json(inputFolder)
    readAll.printSchema()
    readAll.createOrReplaceTempView("Files")
    val data2=mySpark.sql("select text from Files")
    //val d = readFull.select('text).collect().map(rows => rows.getString(0))//.map(line => line.replaceAll(regex2, ""))
    val ds2 = data2.map(rows => rows.getString(0)).rdd
    val counts2 = ds2.flatMap(line => line.split(" "))
      .filter(word => !(word matches regex2)) //удаляем все слова с пунктуацией, если найдете норм способ удалить только пунктуацию будет круто, но необязательно
      //.map(line => line.replaceAll(regex2,"")) //работает не очень, вставляет пустые символы
      //.filter(word => !(word matches ""))
      .map(word => (word, 1))
      .reduceByKey(_ + _)
      .sortBy(- _._2)
      .map(f => f._1 +"\t"+ f._2)*/

    //counts2.saveAsTextFile(outputFolder)



    /* другой способ
    val sqlContext = new SQLContext(sc)
    val df = sqlContext.read.json(inputPath)
    df.printSchema()
    df.registerTempTable("JSONdata")
    val data=sqlContext.sql("select title from JSONdata")
    sc.stop*/
    //val inputPath = args(0)
    //val rawJson: String = {"foo": "bar","baz": 123,"list of stuff": [ 4, 5, 6 ]}"""
    //val parseResult = parse(rawJson).getOrElse(Json.Null)
    //println(parseResult)
  }
}
